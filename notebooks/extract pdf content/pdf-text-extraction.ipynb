{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF text extraction tool\n",
    "Author: Roald Teunissen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import os\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), '../../data/')\n",
    "\n",
    "PAPERS_DIR = os.path.join(data_dir, 'external/papers')\n",
    "PAPERS_DATA_DIR__RAW = os.path.join(data_dir, 'raw', 'papers_raw.csv')\n",
    "PAPERS_DATA_DIR__PROCESSED = os.path.join(data_dir, 'processed', 'papers_processed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extract content from PDF and save as raw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset based on files in *directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_paper_data = []\n",
    "\n",
    "# Loop through all topic folders\n",
    "for topic in os.listdir(PAPERS_DIR): \n",
    "    \n",
    "    # Loop individual files\n",
    "    for filename in os.listdir(os.path.join(PAPERS_DIR, topic)):\n",
    "        file_dir = os.path.join(PAPERS_DIR, topic, filename)\n",
    "\n",
    "        # Only execute when it's a file\n",
    "        if os.path.isfile(file_dir):\n",
    "            pdf_file = open(file_dir,'rb')\n",
    "            \n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            n_pages = len(pdf_reader.pages)\n",
    "            \n",
    "            # Build str object to extract all the content from the pdf\n",
    "            text = ''\n",
    "            for i in range(0, n_pages):\n",
    "                page = pdf_reader.pages[i]\n",
    "                text += page.extract_text()\n",
    "\n",
    "            filename = filename[:-4] # Remove .pdf file extension\n",
    "            raw_paper_data.append({'title': filename, 'topic': topic, 'content': text})\n",
    "\n",
    "df_papers = pd.DataFrame(raw_paper_data, columns = ['title', 'topic', 'content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1D N-type SnO2 nanofibers coexisted with P-typ...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>\\n 263 Psychiatria Danubina, 2022; Vol. 34, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title      topic  \\\n",
       "count                                                  66         66   \n",
       "unique                                                 66          2   \n",
       "top     1D N-type SnO2 nanofibers coexisted with P-typ...  chemistry   \n",
       "freq                                                    1         33   \n",
       "\n",
       "                                                  content  \n",
       "count                                                  66  \n",
       "unique                                                 65  \n",
       "top      \\n 263 Psychiatria Danubina, 2022; Vol. 34, N...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chemistry', 'life sciences and biomedicine'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1D N-type SnO2 nanofibers coexisted with P-typ...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>Content from this work may be used under the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A reconfigurable integrated electronic tongue ...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>arXiv:2205.15018v1  [cs.LG]  27 May 2022\\n© 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMixedMethods Research Agenda to Identify Unde...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>A Mixed Methods Research Agenda to Identify Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computational study of Cu2+, Fe2+, Fe3+, Mn2+ ...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>Content from this work may be used under the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Corrosion inhibitory properties of La0.5Ca0.5M...</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>Content from this work may be used under the t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      topic  \\\n",
       "0  1D N-type SnO2 nanofibers coexisted with P-typ...  chemistry   \n",
       "1  A reconfigurable integrated electronic tongue ...  chemistry   \n",
       "2  AMixedMethods Research Agenda to Identify Unde...  chemistry   \n",
       "3  Computational study of Cu2+, Fe2+, Fe3+, Mn2+ ...  chemistry   \n",
       "4  Corrosion inhibitory properties of La0.5Ca0.5M...  chemistry   \n",
       "\n",
       "                                             content  \n",
       "0  Content from this work may be used under the t...  \n",
       "1  arXiv:2205.15018v1  [cs.LG]  27 May 2022\\n© 20...  \n",
       "2  A Mixed Methods Research Agenda to Identify Un...  \n",
       "3  Content from this work may be used under the t...  \n",
       "4  Content from this work may be used under the t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.to_csv(PAPERS_DATA_DIR__RAW, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load dataset if the variable is not available\n",
    "# Load when file is found\n",
    "if os.path.isfile(PAPERS_DATA_DIR__RAW):\n",
    "    df_papers = pd.read_csv(PAPERS_DATA_DIR__RAW)\n",
    "else:\n",
    "    raise FileNotFoundError('Raw papers dataset is not found')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase and remove punctuations and characters and then strip\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = text.strip()  # get rid of leading/trailing whitespace\n",
    "    text = re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  # replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  # remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) # [0-9] matches any digit (0 to 10000...)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) # matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) # \\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon-based text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword removal\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "# Stemming\n",
    "snow = SnowballStemmer('english')\n",
    "def stemming(string):\n",
    "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
    "    return \" \".join(a)\n",
    "\n",
    "# Lemmatization\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "# Final processing step\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial preprocessing steps over the content from the papers\n",
    "df_papers['content'] = df_papers['content'].apply(lambda x: finalpreprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.to_csv(PAPERS_DATA_DIR__PROCESSED, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-text-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a4ce1929897f4fc22774caf51f2dd777e19a466651e718034b31bf7871224ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
